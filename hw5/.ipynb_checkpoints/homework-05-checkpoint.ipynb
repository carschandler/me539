{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5\n",
    "\n",
    "## References\n",
    "\n",
    "+ Lectures 17-20 (inclusive).\n",
    "\n",
    "## Instructions\n",
    "\n",
    "+ Type your name and email in the \"Student details\" section below.\n",
    "+ Develop the code and generate the figures you need to solve the problems using this notebook.\n",
    "+ For the answers that require a mathematical proof or derivation you should type them using latex. If you have never written latex before and you find it exceedingly difficult, we will likely accept handwritten solutions.\n",
    "+ The total homework points are 100. Please note that the problems are not weighed equally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=3)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(rc={\"figure.dpi\":100, \"savefig.dpi\":300})\n",
    "sns.set_context(\"notebook\")\n",
    "sns.set_style(\"ticks\")\n",
    "\n",
    "import scipy\n",
    "import scipy.stats as st\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "def download(\n",
    "    url : str,\n",
    "    local_filename : str = None\n",
    "):\n",
    "    \"\"\"Download a file from a url.\n",
    "    \n",
    "    Arguments\n",
    "    url            -- The url we want to download.\n",
    "    local_filename -- The filemame to write on. If not\n",
    "                      specified \n",
    "    \"\"\"\n",
    "    if local_filename is None:\n",
    "        local_filename = os.path.basename(url)\n",
    "    urllib.request.urlretrieve(url, local_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student details\n",
    "\n",
    "+ **First Name:** Robert\n",
    "+ **Last Name:** Chandler\n",
    "+ **Email:** chandl71@purdue.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 - Clustering Uber Pickup Data\n",
    "\n",
    "In this problem you will analyze Uber pickup data collected during April 2014 around New York City.\n",
    "The complete data are freely on [Kaggle](https://www.kaggle.com/fivethirtyeight/uber-pickups-in-new-york-city/).\n",
    "The data consist of a timestamp (which we are going to ignore), the latitude and longitude of the Uber pickup, and a base code (which we are also ignoring).\n",
    "The data file we are going to use is [uber-raw-data-apr14.csv](https://raw.githubusercontent.com/PredictiveScienceLab/data-analytics-se/master/homework/uber-raw-data-apr14.csv).\n",
    "As usual, you have to make it visible to this Jupyter notebook.\n",
    "On Google Colab, just run this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://github.com/PredictiveScienceLab/data-analytics-se/raw/master/lecturebook/data/uber-raw-data-apr14.csv\"\n",
    "download(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you can load it using pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "p1_data = pd.read_csv('uber-raw-data-apr14.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how the data look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p1_data.index = pd.to_datetime(p1_data['Date/Time'])\n",
    "# p1_data = p1_data.drop(columns='Date/Time')\n",
    "p1_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have never played before with pandas, you can find a nice tutorial [here](https://purduemechanicalengineering.github.io/me-297-intro-to-data-science/lecture03/python-pandas.html).\n",
    "\n",
    "We have half a million data points. Let's extract the latitude and longitude:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_data = p1_data[['Lon', 'Lat']]\n",
    "loc_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize these points on the map of New York City:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://github.com/PredictiveScienceLab/data-analytics-se/raw/master/lecturebook/images/ny_map.png\"\n",
    "download(url)\n",
    "ny_map = plt.imread('ny_map.png')\n",
    "# ny_map = plt.imread('./hiresmap.png')\n",
    "box = ((loc_data.Lon.min(), loc_data.Lon.max(),\n",
    "        loc_data.Lat.min(), loc_data.Lat.max()))\n",
    "fig, ax = plt.subplots(dpi=200)\n",
    "ax.scatter(\n",
    "    loc_data.Lon,\n",
    "    loc_data.Lat,\n",
    "    zorder=1,\n",
    "    alpha= 0.5,\n",
    "    c='b',\n",
    "    s=0.001\n",
    ")\n",
    "ax.set_xlim(box[0],box[1])\n",
    "ax.set_ylim(box[2],box[3])\n",
    "ax.imshow(\n",
    "    ny_map,\n",
    "    zorder=0,\n",
    "    extent=box,\n",
    "    aspect= 'equal'\n",
    ")\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "sns.despine(trim=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning algorithms will be a bit slow because we have over half a million data points.\n",
    "So, as you develop your code, use only 50K observations.\n",
    "Once you have a stable version of your code, modify the following code segment to use the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color: red\">FIXME</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_train_data = loc_data[:50_000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A - Splitting New York City into Subregions\n",
    "\n",
    "Suppose you are assigned to split New York City into operating subregions with equal demand.\n",
    "When a pickup is requested in each subregion, only the drivers in that region are called.\n",
    "Note that this can become a challenging problem very quickly.\n",
    "We are not looking for the best possible answer here.\n",
    "We are looking for a data-informed heuristic solution that is good enough.\n",
    "\n",
    "Do (at least) the following:\n",
    "+ Use Kmeans clustering on the pickup data with different numbers of clusters;\n",
    "+ Visualize the labels of the clusters on the map using different colors (see the hands-on activities);\n",
    "+ Visualize the centers of the discovered Kmeans clusters (in red color);\n",
    "+ Use common sense, e.g., ensure there are enough clusters so no region crosses the water. If it is impossible to get perfect results simply by Kmeans, feel free to ignore a small number of outliers as they could be handled manually;\n",
    "+ Use [MiniBatchKMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html#sklearn.cluster.MiniBatchKMeans), which is a much faster version of Kmeans suitable for large datasets (>10K observations);\n",
    "\n",
    "Answer with as many text and code blocks as you like below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.cluster\n",
    "import sklearn.ensemble\n",
    "\n",
    "pct_outliers=5\n",
    "\n",
    "outliers = sklearn.ensemble.IsolationForest(random_state=0, contamination=pct_outliers/100).fit_predict(p1_train_data)\n",
    "\n",
    "fig, ax = plt.subplots(dpi=200)\n",
    "\n",
    "ax.set_xlim(box[0],box[1])\n",
    "ax.set_ylim(box[2],box[3])\n",
    "ax.imshow( \n",
    "    ny_map,\n",
    "    zorder=0,\n",
    "    extent=box,\n",
    "    aspect= 'equal'\n",
    ")\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "\n",
    "for i in [1, -1]:\n",
    "    ax.plot(\n",
    "        p1_train_data.loc[outliers == i, 'Lon'],\n",
    "        p1_train_data.loc[outliers == i, 'Lat'],\n",
    "        '.',\n",
    "        zorder=1,\n",
    "        alpha=0.3,\n",
    "        markersize=1,\n",
    "        label='Inliers' if i == 1 else 'Outliers'\n",
    "    )\n",
    "\n",
    "ax.legend();\n",
    "\n",
    "print(f\"Of the original {len(p1_train_data)} points, {np.sum(outliers == -1)} ({pct_outliers}%) have been classified as outliers and discarded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.cluster\n",
    "import sklearn.ensemble\n",
    "n_clusters = 20\n",
    "\n",
    "# Eliminate the top 5% most extreme outliers\n",
    "outliers: np.ndarray = sklearn.ensemble.IsolationForest(random_state=0, contamination=0.025).fit_predict(p1_train_data)\n",
    "\n",
    "p1_train_data_inliers = p1_train_data[outliers == 1]\n",
    "\n",
    "newbox = ((p1_train_data_inliers.Lon.min(), p1_train_data_inliers.Lon.max(),\n",
    "        p1_train_data_inliers.Lat.min(), p1_train_data_inliers.Lat.max()))\n",
    "\n",
    "kmeans = sklearn.cluster.MiniBatchKMeans(n_clusters=n_clusters, batch_size=1536, n_init='auto').fit(p1_train_data_inliers)\n",
    "\n",
    "fig, ax = plt.subplots(dpi=200)\n",
    "\n",
    "ax.set_xlim(newbox[0],newbox[1])\n",
    "ax.set_ylim(newbox[2],newbox[3])\n",
    "\n",
    "ax.imshow( \n",
    "    ny_map,\n",
    "    zorder=0,\n",
    "    extent=box,\n",
    "    aspect= 'equal'\n",
    ")\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    ax.plot(\n",
    "        p1_train_data_inliers.loc[kmeans.labels_ == i, 'Lon'],\n",
    "        p1_train_data_inliers.loc[kmeans.labels_ == i, 'Lat'],\n",
    "        'o',\n",
    "        # fillstyle='none',\n",
    "        zorder=1,\n",
    "        alpha=0.3,\n",
    "        markersize=0.5,\n",
    "        # mew=0.5,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=200)\n",
    "\n",
    "ax.imshow( \n",
    "    ny_map,\n",
    "    zorder=0,\n",
    "    extent=box,\n",
    "    aspect= 'equal'\n",
    ")\n",
    "ax.set_xlabel('Longitude')\n",
    "ax.set_ylabel('Latitude')\n",
    "ax.set_title('Cluster Centers')\n",
    "\n",
    "ax.plot(\n",
    "    kmeans.cluster_centers_[:, 0],\n",
    "    kmeans.cluster_centers_[:, 1],\n",
    "    'r+',\n",
    "    markersize = 5,\n",
    "    markeredgewidth=0.7,\n",
    "    alpha=1,\n",
    "    # fillstyle='none',\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B - Create a Stochastic Model of Pickups\n",
    "\n",
    "One of the key ingredients for a more sophisticated approach to optimizing the operations of Uber is the construction of a stochastic model of the demand for pickups.\n",
    "The ideal model for this problem is the [Poisson Point Process](https://en.wikipedia.org/wiki/Poisson_point_process).\n",
    "However, we will do something more straightforward, using the Gaussian mixture model and a Poisson random variable.\n",
    "The model will not have a time component, but it will allow us to sample the number and locations of pickups during a typical month.\n",
    "We will guide you through the process of constructing this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subpart B.I - Random variable capturing the number of monthly pickups\n",
    "\n",
    "Find the rate of monthly pickups (ignore the fact that months may differ by a few days) and use it to define a Poisson random variable corresponding to the monthly number of pickups.\n",
    "Use ``scipy.stats.poisson`` to initialize this random variable. Sample from it 10,000 times and plot the histogram of the samples to get a feeling about the corresponding probability mass function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_rate = len(loc_data)\n",
    "monthly_pickups_dist = st.poisson(mu=monthly_rate)\n",
    "monthly_samples = monthly_pickups_dist.rvs(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.hist(monthly_samples, density=True);\n",
    "ax.set_ylabel(\"Probability Density\")\n",
    "ax.set_xlabel(\"Number of Pickups in 1 Month\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.mixture\n",
    "gm = sklearn.mixture.GaussianMixture(n_components=20).fit(p1_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subpart B.II - Sample some random monthly pickup numbers\n",
    "\n",
    "Now that you have a model that gives you the number of pickups and a model that allows you to sample a pickup location, sample five different datasets (number of pickups and location of each pick) from the combined model and visualize them on the New York map.\n",
    "\n",
    "**Hint:** Don't get obsessed with making the model perfect. It's okay if a few of the pickups are on water."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_pickups in monthly_pickups_dist.rvs(5):\n",
    "    location_samples, _ = gm.sample(n_pickups)\n",
    "    \n",
    "    fig, ax = plt.subplots(dpi=200)\n",
    "    ax.imshow( \n",
    "        ny_map,\n",
    "        zorder=0,\n",
    "        extent=box,\n",
    "        aspect= 'equal'\n",
    "    )\n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "    ax.set_title('Cluster Centers')\n",
    "    \n",
    "    ax.plot(\n",
    "        location_samples[:, 0],\n",
    "        location_samples[:, 1],\n",
    "        '.',\n",
    "        zorder=1,\n",
    "        alpha=0.3,\n",
    "        markersize=1,\n",
    "    );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 - Counting Celestial Objects\n",
    "\n",
    "Consider this picture of a patch of sky taken by the [Hubble Space Telescope](https://www.nasa.gov/mission_pages/hubble/story/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download it so that you have it here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/PredictiveScienceLab/data-analytics-se/master/lecturebook/images/galaxies.png'\n",
    "download(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This picture includes many galaxies but also some stars.\n",
    "We will create a machine-learning model capable of counting the number of objects in such images.\n",
    "Our model will not be able to differentiate between the different types of objects and will not be very accurate. Still, it does form the basis of more sophisticated approaches.\n",
    "The idea is as follows:\n",
    "+ Convert the picture to points sampled according to the intensity of light.\n",
    "+ Apply Gaussian mixture on the resulting points.\n",
    "+ Use the Bayesian Information Criterion to identify the number of components in the picture.\n",
    "+ Associate the number of components with the actual number of celestial objects.\n",
    "\n",
    "I will set you up with the first step. You will have to do the last three."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to load the image with the [Python Imaging Library (PIL)](https://en.wikipedia.org/wiki/Python_Imaging_Library), which allows us to apply a few basic transformations to the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "hubble_image = Image.open('galaxies.png')\n",
    "# here is how to see the image\n",
    "hubble_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to convert it to grayscale and crop it to make the problem a little bit easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = hubble_image.convert('L').crop((100, 100, 300, 300))\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that black-and white images are matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_ar = np.array(img)\n",
    "img_ar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minimum number is $0$, corresponding to black, and the maximum is 255, corresponding to white.\n",
    "Anything in between is some shade of gray."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, imagine that each pixel is associated with some coordinates.\n",
    "Without loss of generality, let's assume that each pixel is some coordinate in $[0,1]^2$.\n",
    "We will loop over each pixel and sample its coordinates in a way that increases with increasing light intensity.\n",
    "To achieve this, we will pass the intensity values of each pixel through a sigmoid with parameters that can be tuned.\n",
    "Here is this sigmoid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intensities = np.linspace(0, 255, 255)\n",
    "fig, ax = plt.subplots()\n",
    "alpha = 0.1\n",
    "beta = 255 / 3\n",
    "ax.plot(\n",
    "    intensities,\n",
    "    1.0 / (1.0 + np.exp(-alpha * (intensities - beta)))\n",
    ");\n",
    "ax.set_xlabel('Light intensities')\n",
    "ax.set_ylabel('Probability of sampling the pixel coordinates')\n",
    "sns.despine(trim=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is the code that samples the pixel coordinates.\n",
    "I am organizing it into a function because we may want to use it with different pictures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_pixel_coords(img, alpha, beta):\n",
    "    \"\"\"\n",
    "    Samples pixel coordinates based on a probability defined as the sigmoid of the intensity.\n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "        img    -     The gray scale picture from which we sample as an array\n",
    "        alpha     -     The scale of the sigmoid\n",
    "        beta      -     The offset of the sigmoid\n",
    "    \"\"\"\n",
    "    img_ar = np.array(img)\n",
    "    x = np.linspace(0, 1, img_ar.shape[0])\n",
    "    y = np.linspace(0, 1, img_ar.shape[1])\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    img_to_locs = []\n",
    "    # Loop over pixels\n",
    "    for i in range(img_ar.shape[1]):\n",
    "        for j in range(img_ar.shape[0]):\n",
    "            # Calculate the probability of the pixel by looking at each\n",
    "            # light intensity\n",
    "            prob = 1.0 / (1.0 + np.exp(-alpha * (img_ar[j, i] - beta)))\n",
    "            # Pick a uniform random number\n",
    "            u = np.random.rand()\n",
    "            # If u is smaller than the desired probability,\n",
    "            # the consider the coordinates of the pixel sampled\n",
    "            if u <= prob:\n",
    "                img_to_locs.append((Y[i, j], X[-i-1, -j-1]))\n",
    "    # Turn img_to_locs into a numpy array\n",
    "    img_to_locs = np.array(img_to_locs)\n",
    "    return img_to_locs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locs = sample_pixel_coords(img, alpha=0.1, beta=200)\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.imshow(img, extent=((0, 1, 0, 1)), zorder=0)\n",
    "ax.scatter(\n",
    "    locs[:, 0],\n",
    "    locs[:, 1],\n",
    "    zorder=1,\n",
    "    alpha=0.5,\n",
    "    c='b',\n",
    "    s=1\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that playing with $\\alpha$ and $\\beta$ makes the whole thing more or less sensitive to the light intensity.\n",
    "\n",
    "Complete the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "def count_objs(img, alpha, beta, nc_min=1, nc_max=50, random_state=0):\n",
    "    \"\"\"Count objects in image.\n",
    "    \n",
    "    Arguments:\n",
    "        img       -     The image\n",
    "        alpha     -     The scale of the sigmoid\n",
    "        beta      -     The offset of the sigmoid\n",
    "        nc_min    -     The minimum number of components to consider\n",
    "        nc_max    -     The maximum number of components to consider\n",
    "    \"\"\"\n",
    "    locs = sample_pixel_coords(img, alpha, beta)\n",
    "    # **** YOUR CODE HERE ****\n",
    "    # Use BIC to search for the best GaussianMixture model\n",
    "    # with components between nc_min and nc_max\n",
    "    # YOU CAN PULL THIS OFF BY COPY-PASTING MATERIAL FROM\n",
    "    # LECTURE 17\n",
    "    # Set the following variables\n",
    "    models = list()\n",
    "    bics = np.zeros(50)\n",
    "\n",
    "    for i, n_components in enumerate(range(nc_min, nc_max + 1)):\n",
    "        model = GaussianMixture(n_components=n_components, random_state=random_state).fit(locs)\n",
    "        models.append(model)\n",
    "        bics[i] = model.bic(locs)\n",
    "\n",
    "    # print(bics)\n",
    "    # print(np.argmin(bics))\n",
    "    \n",
    "    best_nc = np.argmin(bics) + 1\n",
    "    best_model = models[np.argmin(bics)]\n",
    "    return best_nc, best_model, locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_objs(img, alpha, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have completed the code, try out the following images.\n",
    "Feel free to play with $\\alpha$ and $\\beta$ to improve the performance.\n",
    "**Do not try to make a perfect model. We would have to go beyond the Gaussian mixture model to do so. This is just a homework problem.**\n",
    "\n",
    "Here is a helpful function that you can use to visualize the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_counts(img, objs, model, locs):\n",
    "    \"\"\"Visualize the counts.\n",
    "    \n",
    "    Arguments\n",
    "    img    --  The image.\n",
    "    objs   --  Returned by count_objs()\n",
    "    model  --  Returned by count_objs()\n",
    "    locs   --  Returned by count_objs()\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(dpi=150)\n",
    "    ax.imshow(img, extent=((0, 1, 0, 1)))\n",
    "    for i in range(model.means_.shape[0]):\n",
    "        ax.plot(\n",
    "            model.means_[i, 0],\n",
    "            model.means_[i, 1],\n",
    "            'gx', \n",
    "            markersize=(\n",
    "                10.0 * model.weights_.shape[0]\n",
    "                * model.weights_[i]\n",
    "            )\n",
    "        )\n",
    "    ax.scatter(\n",
    "        locs[:, 0],\n",
    "        locs[:, 1],\n",
    "        zorder=1,\n",
    "        alpha=0.5,\n",
    "        c='b',\n",
    "        s=1\n",
    "    )\n",
    "    ax.set_title('Counted {0:d} objects!'.format(objs));  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how to use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objs, model, locs = count_objs(img, alpha=1.0, beta=200)\n",
    "visualize_counts(img, objs, model, locs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try this image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = hubble_image.convert('L').crop((200, 200, 400, 400))\n",
    "objs, model, locs = count_objs(img, alpha=.1, beta=250)\n",
    "visualize_counts(img, objs, model, locs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = hubble_image.convert('L').crop((300, 300, 500, 500))\n",
    "objs, model, locs = count_objs(img, alpha=.1, beta=250)\n",
    "visualize_counts(img, objs, model, locs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3 - Filtering of an Oscillator with Damping\n",
    "\n",
    "Assume that you are dealing with a one-degree-of-freedom system which follows the equation:\n",
    "\n",
    "$$\n",
    "\\ddot{x} + 2\\zeta \\omega_0 \\dot{x} + \\omega^2_0 x = u_0 \\cos(\\omega t),\n",
    "$$\n",
    "\n",
    "where $x = x(t)$ is the generalized coordinate of the oscillator at time $t$, and the parameters $\\zeta$, $\\omega_0$, $u_0$, and $\\omega$ are known to you (we will give them specific values later).\n",
    "Furthermore, assume that you are making noisy observations of the *absolute acceleration* at discrete timesteps $\\Delta t$ (also known):\n",
    "\n",
    "$$\n",
    "y_j = \\ddot{x}(j\\Delta t)-u_0 \\cos(\\omega t)+w_j,\n",
    "$$\n",
    "\n",
    "for $j=1,\\dots,n$, where $w_j \\sim N(0, \\sigma^2)$ with $\\sigma^2$ also known.\n",
    "Finally, assume that the initial conditions for the position and the velocity (you need both to get a unique solution) are given by:\n",
    "\n",
    "$$\n",
    "x_0 = x(0) \\sim N(0, \\sigma_x^2),\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "v_0 = \\dot{x} \\sim N(0, \\sigma_v^2).\n",
    "$$\n",
    "\n",
    "Of course, assume that $\\sigma_x^2$ and $\\sigma_v^2$ are specific numbers we will specify below.\n",
    "\n",
    "Before we go over the questions, let's write code that generates the actual trajectory of the system at some random initial conditions and some observations.\n",
    "We will use the code to generate a synthetic dataset with known ground truth, which you will use in your filtering analysis.\n",
    "\n",
    "The first step we need to do is to turn the problem into a first-order differential equation.\n",
    "We set:\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = \n",
    "\\begin{bmatrix}\n",
    "x\\\\\n",
    "\\dot{x}\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "Assuming $\\mathbf{x} = (x_1,x_2)$, then the dynamics are described by:\n",
    "\n",
    "$$\n",
    "\\dot{\\mathbf{x}} = \n",
    "\\begin{bmatrix}\n",
    "\\dot{x}\\\\\n",
    "\\ddot{x}\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "x_2\\\\\n",
    "-2\\zeta \\omega_0 \\dot{x} - \\omega^2_0 x + u_0 \\cos(\\omega t)\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "x_2\\\\\n",
    "-2\\zeta \\omega_0 x_2 - \\omega^2_0 x_1 + u_0 \\cos(\\omega t)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The initial conditions are of course, just:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_0 =\n",
    "\\begin{bmatrix}\n",
    "x_0\\\\\n",
    "v_0\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "This first-order system can solved using [scipy.integrate.solve_ivp](https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.solve_ivp.html#scipy.integrate.solve_ivp).\n",
    "Here is how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.integrate import solve_ivp\n",
    "\n",
    "# You need to define the right hand side of the equation\n",
    "def rhs(t, x, omega0, zeta, u0, omega):\n",
    "    \"\"\"Return the right hand side of the dynamical system.\n",
    "    \n",
    "    Arguments\n",
    "    t       -    Time\n",
    "    x       -    The state\n",
    "    omega0  -    Natural frequency\n",
    "    zeta    -    Dumping factor (0<=zeta)\n",
    "    u0      -    External force amplitude\n",
    "    omega   -    Excitation frequency\n",
    "    \"\"\"\n",
    "    res = np.ndarray((2,))\n",
    "    res[0] = x[1]\n",
    "    res[1] = -2.0 * zeta * omega0 * x[1] - omega0 ** 2 * x[0] + u0 * np.cos(omega * t)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is how you solve it for given initial conditions and parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial conditions\n",
    "x0 = np.array([0.0, 1.0])\n",
    "# Natural frequency\n",
    "omega0 = 2.0\n",
    "# Dumping factor\n",
    "zeta = 0.4\n",
    "# External forcing amplitude\n",
    "u0 = 0.5\n",
    "# Excitation frequency\n",
    "omega = 2.1\n",
    "# Timestep\n",
    "dt = 0.1\n",
    "# The final time\n",
    "final_time = 10.0\n",
    "# The number of timesteps to get the final time\n",
    "n_steps = int(final_time / dt)\n",
    "# The times on which you want the solution\n",
    "t_eval = np.linspace(0, final_time, n_steps)\n",
    "# The solution\n",
    "sol = solve_ivp(rhs, (0, final_time), x0, t_eval=t_eval, args=(omega0, zeta, u0, omega))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution is stored in the ``sol`` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol.y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of ``sol.y`` is (2, 100), which means that we have 100 timesteps and two variables (position and velocity).\n",
    "Let's plot the position and the velocity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.plot(t_eval, sol.y[0, :], label='Position')\n",
    "ax.plot(t_eval, sol.y[1, :], label='Velocity')\n",
    "ax.set_xlabel('$t$')\n",
    "ax.set_ylabel('$x_i(t)$')\n",
    "plt.legend(loc='best', frameon=False)\n",
    "sns.despine(trim=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now generate some synthetic observations of the acceleration with some given Gaussian noise.\n",
    "To get the acceleration, you can do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute external excitation.\n",
    "us = u0 * np.cos(omega * t_eval)\n",
    "# Subtract us from \\ddot{x}\n",
    "true_acc = np.array([rhs(t, x, omega0, zeta, u0, omega)[1] for (t, x) in zip(t_eval, sol.y.T)])-us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add some noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_r = 0.2\n",
    "observations = true_acc + sigma_r * np.random.randn(true_acc.shape[0])\n",
    "\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.plot(t_eval, true_acc, label='Acceleration')\n",
    "ax.plot(\n",
    "    t_eval,\n",
    "    observations,\n",
    "    '.',\n",
    "    label='Noisy observation of acceleration'\n",
    ")\n",
    "ax.set_xlabel('$t$')\n",
    "ax.set_ylabel(r'$\\ddot{x}(t)$')\n",
    "plt.legend(loc='best', frameon=False)\n",
    "sns.despine(trim=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay. Now, imagine that you only see the noisy observations of the acceleration.\n",
    "The filtering goal is to recover the state of the underlying system (as well as its acceleration).\n",
    "I am going to guide you through the steps you need to follow.\n",
    "\n",
    "## Part A - Discretize time (Transitions)\n",
    "\n",
    "Use the Euler time discretization scheme to turn the continuous dynamical system into a discrete-time dynamical system like this:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_{j+1} = \\mathbf{A}\\mathbf{x}_j + Bu_j + \\mathbf{z}_j,\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_j = \\mathbf{x}(j\\Delta t),\n",
    "$$\n",
    "\n",
    "$$\n",
    "u_j = u(j\\Delta t),\n",
    "$$\n",
    "\n",
    "<p style=\"color: red\"> FIXME </p>\n",
    "$$\n",
    "\\dot{\\mathbf{x}} = \n",
    "\\begin{bmatrix}\n",
    "\\dot{x}\\\\\n",
    "\\ddot{x}\n",
    "\\end{bmatrix}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "x_2\\\\\n",
    "-2\\zeta \\omega_0 \\dot{x} - \\omega^2_0 x + u_0 \\cos(\\omega t)\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "x_2\\\\\n",
    "-2\\zeta \\omega_0 x_2 - \\omega^2_0 x_1 + u_0 \\cos(\\omega t)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and $\\mathbf{z}_j$ is properly chosen process noise term.\n",
    "You should derive and provide mathematical expressions for the following:\n",
    "+ The $2 \\times 2$ transition matrix $\\mathbf{A}$.\n",
    "+ The $2 \\times 1$ control \"matrix\" $B$.\n",
    "+ The process covariance $\\mathbf{Q}$. For the process covariance, you may choose your values by hand.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should be using the parameters dt, omega0, zeta, etc.\n",
    "# from above\n",
    "A = np.array(\n",
    "    [\n",
    "        [0, 1],\n",
    "        [-omega0**2, -2 * zeta * omega0]\n",
    "    ]\n",
    ") * dt\n",
    "\n",
    "B = np.array(\n",
    "    [\n",
    "        [0],\n",
    "        [u0 * np.cos(omega)]\n",
    "    ]\n",
    ") * dt\n",
    "\n",
    "Q = np.array(\n",
    "    [\n",
    "        [1, 0.0],\n",
    "        [0.0, 1]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B - Discretize time (Emissions)\n",
    "\n",
    "Establish the map that takes you from the states to the accelerations at each timestep.\n",
    "That is, specify:\n",
    "\n",
    "$$\n",
    "y_j = \\mathbf{C}\\mathbf{x}_j + w_j,\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "y_j = \\ddot{x}(j\\Delta t)-u_0 \\cos(\\omega t)+w_j,\n",
    "$$\n",
    "\n",
    "and $w_j$ is a measurement noise.\n",
    "You should derive and provide mathematical expressions for the following:\n",
    "+ The $1 \\times 2$ emission matrix $\\mathbf{C}$.\n",
    "+ The $1 \\times 1$ covariance \"matrix\" $R$ of the measurement noise.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.array(\n",
    "    [\n",
    "        [REPLACE_ME, REPLACE_ME]\n",
    "    ]\n",
    ")\n",
    "\n",
    "R = np.array(\n",
    "    [\n",
    "        [REPLACE_ME]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C - Apply the Kalman filter\n",
    "\n",
    "Use ``FilterPy`` (see the hands-on activity of Lecture 20) to infer the unobserved states given the noisy observations of the accelerations.\n",
    "Plot time-evolving 95% credible intervals for the position and the velocity along with the true unobserved values of these quantities (in two separate plots)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here (as many code and text blocks as you want)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part D - Quantify and visualize your uncertainty about the actual acceleration value\n",
    "\n",
    "Use standard uncertainty propagation techniques to quantify your epistemic uncertainty about the true acceleration value.\n",
    "You will have to use the inferred states of the system and the dynamical model.\n",
    "This can be done either analytically or by Monte Carlo. It's your choice.\n",
    "In any case, plot time-evolving 95% credible intervals for the acceleration (epistemic only), the true unobserved values, and the noisy measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here (as many code and text blocks as you want)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
